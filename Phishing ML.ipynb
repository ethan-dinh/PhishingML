{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Machine Learning to Identify Phishing Websites\n",
    "**Authors:** Ethan Dinh, Izzy Harker, Annika Huston <br>\n",
    "**Date:** 9 February 2024 <br>\n",
    "**Class:** CS 433 â€“ Network Security\n",
    "\n",
    "### Goal\n",
    "> The primary goal of leveraging machine learning to identify phishing websites is to develop an automated, efficient, and highly accurate system capable of detecting and classifying websites as legitimate or phishing. This system aims to analyze various characteristics of websites, such as URL structure, site content, and metadata, to identify potential phishing attempts. By continuously learning from new data, the model seeks to improve its detection capabilities over time, adapting to the evolving tactics used by cybercriminals.\n",
    "\n",
    "### Purpose\n",
    "> The purpose of this initiative is to enhance cybersecurity measures by providing a robust tool that can significantly reduce the risk of phishing attacks for individuals and organizations. By automating the detection of phishing websites, the system intends to preemptively block access to these malicious sites, thereby protecting sensitive information from being compromised. Additionally, this project aims to raise awareness about the sophistication of phishing schemes and promote safer online practices among internet users. Ultimately, leveraging machine learning for phishing website identification supports the broader objective of creating a safer, more secure digital environment for all users.\n",
    "\n",
    "# Table of Contents\n",
    "1. [Importing Libraries](#LIBS)\n",
    "2. [Importing & Cleaning Data Files](#FILTER)\n",
    "3. [Feature Selection](#FEATURE)\n",
    "4. [Implementing & Comparing Various Machine Learning Models](#IMPLEMENT)\n",
    "5. [Hyper Parameter Tuning](#TUNE)\n",
    "5. [Extrapolation](#EXTRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Core Libraries\n",
    "<a class=\"anchor\" id=\"LIBS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing & Cleaning Data Files\n",
    "URL: https://www.sciencedirect.com/science/article/pii/S2352340920313202\n",
    "\n",
    "Overview: \n",
    "* These data consist of a collection of legitimate, as well as phishing website instances. Each website is represented by the set of features that denote whether the website is legitimate or not. Data can serve as input for the machine learning process.\n",
    "* Machine learning and data mining researchers can benefit from these datasets, while also computer security researchers and practitioners. Computer security enthusiasts can find these datasets interesting for building firewalls, intelligent ad blockers, and malware detection systems.\n",
    "* This dataset can help researchers and practitioners easily build classification models in systems preventing phishing attacks since the presented datasets feature the attributes which can be easily extracted.\n",
    "* Finally, the provided datasets could also be used as a performance benchmark for developing state-of-the-art machine learning methods for the task of phishing websites classification.\n",
    "\n",
    "<a class=\"anchor\" id=\"FILTER\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the file path of the data files here:\n",
    "filePath = \"./Input Data/\"\n",
    "\n",
    "# Loading in data\n",
    "df = pd.read_csv(filePath + \"dataset_small.csv\")\n",
    "\n",
    "# Remove all null rows\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "<a class=\"anchor\" id=\"FEATURE\"></a>\n",
    "We will be using recursive feature elimination, or RFE as a feature selection algorithm. RFE is a wrapper-type feature selection algorithm which implies that a different machine learning model will be used to help assist in the feature selection process. In general, RFE works by searching for a subset of features by starting with all features in the training dataset and removing features until the desired number remains. This process is done by fitting the given machine learning model, ranking features by importance, and removing the least important features, and re-fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries to pick the best features\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from pyHSICLasso import HSICLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "# Retrieving the features\n",
    "features = df.columns[:-1]\n",
    "X = df[features]\n",
    "Y = df['phishing']\n",
    "\n",
    "# Split DF into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1, train_size = .75)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding LDA and RFE\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA)** is a technique used both for dimensionality reduction and as a classifier. For dimensionality reduction, LDA seeks to find a projection that maximizes the separation between multiple classes.\n",
    "- **Recursive Feature Elimination (RFE)** is a feature selection method that recursively removes features, building the model with the remaining features to identify which ones contribute most to predicting the target variable.\n",
    "\n",
    "### When Combining Them Might Make Sense\n",
    "\n",
    "- **Feature Selection for Classification**: If you're using LDA as a classifier, employing RFE to select features could potentially improve your model's performance by eliminating irrelevant or redundant features that don't contribute to separating the classes.\n",
    "- **Dimensionality Reduction in Multi-step Process**: In scenarios where you're dealing with high-dimensional data but want to use LDA for classification rather than dimensionality reduction, RFE can help reduce dimensionality first, possibly enhancing LDA's classification performance.\n",
    "\n",
    "### Considerations and Alternatives\n",
    "\n",
    "- **Redundancy in Dimensionality Reduction**: LDA inherently performs a form of dimensionality reduction by projecting features onto a lower-dimensional space that maximizes class separability. Using RFE for further dimensionality reduction before LDA might be redundant or unnecessary, especially if the primary goal is dimensionality reduction rather than feature selection.\n",
    "- **Compatibility with LDA Objectives**: If the main goal is to use LDA for its dimensionality reduction capabilities to facilitate another classifier's performance, consider whether RFE aligns with your objectives. RFE is more about selecting features based on their importance, which might or might not align with the goal of maximizing class separability in the reduced-dimensional space.\n",
    "- **Alternatives to RFE**: Depending on your dataset and goals, other feature selection or dimensionality reduction techniques might be more straightforward or effective when combined with LDA. For example, methods like Principal Component Analysis (PCA) for dimensionality reduction (without considering class labels) or model-based feature selection techniques that consider feature importance might be alternatives to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Retrieves a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(10, 26):\n",
    "        rfe = RFE(estimator=LDA(), n_features_to_select=i)\n",
    "        model = DecisionTreeClassifier()\n",
    "        models[str(i)] = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "    return models\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = KFold(n_splits=5)\n",
    "    \n",
    "    # Make sure to use X and y instead of X_train and y_train\n",
    "    y_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1, method='predict')\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and F1 score for a comprehensive evaluation\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    \n",
    "    # Calculate cross-validated accuracy score for comparison, using X and y\n",
    "    cv_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    return {\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV Accuracy Scores': cv_scores,\n",
    "        'CV Accuracy Mean': cv_scores.mean(),\n",
    "        'CV Accuracy Std': cv_scores.std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_num_feats():\n",
    "    # Standardize the dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    models = get_models()\n",
    "    all_results = []\n",
    "    for name, model in models.items():\n",
    "        output_dict = evaluate_model(model, X_scaled, Y)\n",
    "        print('>%s %.3f (%.3f)' % (name, output_dict['CV Accuracy Mean'], output_dict['CV Accuracy Std']))\n",
    "        \n",
    "        # Exclude 'CV Accuracy Scores' from the dictionary\n",
    "        output_dict.pop('CV Accuracy Scores', None)\n",
    "        \n",
    "        # Add the model name or the number of features as a new key-value pair\n",
    "        output_dict['Features'] = name\n",
    "        \n",
    "        all_results.append(output_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    results_df.to_csv('Results/model_evaluation_results.csv', index=False)\n",
    "    \n",
    "# Check if the model evaluation results CSV file exists\n",
    "if not os.path.exists('Results/model_evaluation_results.csv'):\n",
    "    evaluate_num_feats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F1 score are metrics used to evaluate the quality of classification models, especially in the context of imbalanced datasets where accuracy might not provide a complete picture of the model's performance. Here's how to interpret each of these metrics:\n",
    "\n",
    "### Precision\n",
    "- **Definition**: Precision measures the accuracy of the positive predictions. It is the ratio of true positive predictions to the total predicted positives, which includes both true positives and false positives.\n",
    "- **Formula**: Precision = True Positives / (True Positives + False Positives)\n",
    "- **Interpretation**: A high precision score indicates that the model is reliable in its positive predictions, meaning when it predicts a class, it is likely correct. However, a model can achieve high precision by being overly conservative, possibly missing many true positives (i.e., it predicts positive only when very sure).\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "- **Definition**: Recall measures the ability of the model to identify all relevant instances within a dataset. It is the ratio of true positive predictions to the actual positives in the data, which includes both true positives and false negatives.\n",
    "- **Formula**: Recall = True Positives / (True Positives + False Negatives)\n",
    "- **Interpretation**: A high recall score indicates that the model is good at detecting positive instances, capturing a large proportion of actual positives. However, a model can achieve high recall at the cost of making many incorrect positive predictions (i.e., low precision).\n",
    "\n",
    "### F1 Score\n",
    "- **Definition**: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both the precision and recall of a model.\n",
    "- **Formula**: F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- **Interpretation**: The F1 score considers both false positives and false negatives, providing a more comprehensive view of model performance, especially in imbalanced datasets. A high F1 score indicates not only that the model accurately identifies positive instances but also that it does so without significantly increasing the false positive rate. It's particularly useful when you need a balance between precision and recall.\n",
    "\n",
    "### Practical Considerations\n",
    "- **Precision vs. Recall Trade-off**: Increasing precision typically reduces recall and vice versa. The importance of precision versus recall varies depending on the application. For example, in spam detection (where missing a spam email is preferable to wrongly marking an important email as spam), precision may be more important. In contrast, in disease screening, a high recall might be preferred to ensure as few cases as possible are missed, even if it means some false positives.\n",
    "- **Choosing a Metric**: The choice of metric depends on the specific requirements of your application and the cost of false positives versus false negatives. In some cases, other metrics like specificity (true negative rate) or ROC AUC might also be relevant.\n",
    "\n",
    "Understanding these metrics and their implications can help in fine-tuning the model and in choosing the right evaluation strategy for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_evaluation_results_sns(csv_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert the 'Features' column to numeric for plotting\n",
    "    df['Features'] = pd.to_numeric(df['Features'])\n",
    "    \n",
    "    # Sort the DataFrame based on the number of features\n",
    "    df.sort_values('Features', inplace=True)\n",
    "\n",
    "    # Initialize the matplotlib figure\n",
    "    plt.figure(figsize=(7, 4))\n",
    "\n",
    "    # Set the seaborn theme\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Plot Accuracy\n",
    "    sns.lineplot(data=df, x='Features', y='Accuracy', marker='o', label='Accuracy')\n",
    "    \n",
    "    # Plot Precision\n",
    "    sns.lineplot(data=df, x='Features', y='Precision', marker='s', label='Precision')\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    sns.lineplot(data=df, x='Features', y='F1 Score', marker='^', label='F1 Score')\n",
    "\n",
    "    # Finalizing the plot\n",
    "    plt.title('Model Evaluation Metrics by Number of Features')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Metric Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Increase the font size\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_evaluation_results_sns('Results/model_evaluation_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HSIC Lasso (Hilbert-Schmidt Independence Criterion Least Absolute Shrinkage and Selection Operator) is a non-linear feature selection method that can be particularly powerful in scenarios where the relationship between features and the target variable is complex and not well-captured by linear models. It is designed to capture both linear and non-linear dependencies between variables using kernel methods, making it a versatile tool in the feature selection arsenal, especially for high-dimensional data.\n",
    "\n",
    "### When HSIC Lasso Excels:\n",
    "\n",
    "- **Non-linear Relationships:** HSIC Lasso can detect non-linear relationships between features and the target, outperforming linear methods like traditional LASSO or linear RFE when such relationships are present.\n",
    "- **High-dimensional Data:** It's well-suited for scenarios where you have a large number of features, potentially with complex interdependencies.\n",
    "- **Feature Relevance and Redundancy:** HSIC Lasso can help identify a subset of relevant features while considering their redundancy, aiming to select features that individually and collectively have strong predictive power.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Computational Complexity:** HSIC Lasso's computational complexity can be higher than simpler linear methods, especially as the number of features grows, which might pose challenges for very large datasets.\n",
    "- **Implementation and Interpretability:** Implementing HSIC Lasso might require more effort, and the results might be less interpretable to stakeholders unfamiliar with kernel methods compared to more straightforward linear models.\n",
    "- **Dependency on Hyperparameters:** Like many machine learning methods, HSIC Lasso's performance depends on the choice of hyperparameters, such as the kernel type and its parameters. Selecting these requires careful tuning and validation.\n",
    "\n",
    "### Comparing with RFE and LDA:\n",
    "\n",
    "- **RFE (Recursive Feature Elimination)** with a linear model like LDA (Linear Discriminant Analysis) focuses on linear relationships and iteratively removes features to find an optimal subset. RFE can be very effective when the relationship between features and the target is approximately linear or when interpretability and computational efficiency are priorities.\n",
    "- **HSIC Lasso** could potentially be a better choice when you suspect significant non-linear interactions between your features and the target variable, and you are dealing with high-dimensional data where capturing these complex relationships could lead to better model performance.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Whether HSIC Lasso is a better feature selection model for your specific problem depends on the nature of your dataset and the problem you're trying to solve. If non-linearity, high dimensionality, and complex feature interactions characterize your data, HSIC Lasso could offer significant advantages. However, for problems where linear relationships predominate or when simplicity, speed, and interpretability are key considerations, traditional methods like RFE with LDA or other linear models might be more appropriate.\n",
    "\n",
    "It's often beneficial to experiment with different feature selection methods, including HSIC Lasso and linear approaches, to empirically determine which method works best for your specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standard scaler to the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Merge the training set and the target variable\n",
    "train_df = pd.DataFrame(X_train_scaled, columns=features)\n",
    "train_df['phishing'] = y_train\n",
    "\n",
    "# Save the training set to a CSV file\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "\n",
    "# Testing with HSIC Lasso for Feature Selection\n",
    "def hsic_lasso_feature_selection():\n",
    "    hsic_lasso = HSICLasso()    \n",
    "    hsic_lasso.input(\"train.csv\", output_list = [\"phishing\"])\n",
    "    hsic_lasso.classification(20, 5, 2)\n",
    "    hsic_lasso.dump()\n",
    "    hsic_lasso.save_param()\n",
    "    \n",
    "    # Move the parameter file to the Results folder\n",
    "    os.rename('param.csv', 'Results/param.csv')\n",
    "    \n",
    "    # Remove the training set CSV file\n",
    "    os.remove('train.csv')\n",
    "\n",
    "if not os.path.exists('Results/param.csv'):\n",
    "    hsic_lasso_feature_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv('Results/param.csv')\n",
    "HSIC_feats = params[params.columns[0]].tolist()\n",
    "\n",
    "print(HSIC_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing & Comparing Various Machine Learning Models\n",
    "<a class=\"anchor\" id=\"IMPLEMENT\"></a>\n",
    "Defining functions to tune the feature selection algorithm. Each model that is tested will be evaluated via cross-validation to ensure that the results are not due to chance alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "def test_models_with_cv_metrics(X, y, n_splits=5):\n",
    "    # Define Stratified K-Fold cross-validator\n",
    "    cv = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    # List of models to test\n",
    "    models = [\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        SVC(probability=True),  # Enable probability for SVC\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        CatBoostClassifier(verbose=0)  # Suppressing verbose output\n",
    "    ]\n",
    "    \n",
    "    model_names = ['Logistic Regression', 'Random Forest', 'SVC', 'XGBClassifier', 'CatBoostClassifier']\n",
    "    results = []\n",
    "\n",
    "    # Define metrics to evaluate\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model, name in zip(models, model_names):\n",
    "        pipeline = make_pipeline(StandardScaler(), model)\n",
    "        \n",
    "        # Calculate cross-validated scores for all metrics\n",
    "        scores = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=2)\n",
    "        \n",
    "        # Calculate mean scores and standard deviation\n",
    "        mean_accuracy = np.mean(scores['test_accuracy'])\n",
    "        std_accuracy = np.std(scores['test_accuracy'])\n",
    "        mean_precision = np.mean(scores['test_precision_weighted'])\n",
    "        mean_recall = np.mean(scores['test_recall_weighted'])\n",
    "        mean_f1 = np.mean(scores['test_f1_weighted'])\n",
    "        \n",
    "        print(f'{name} CV Accuracy: {mean_accuracy:.3f} ({std_accuracy:.3f})')\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'CV Accuracy Mean': mean_accuracy,\n",
    "            'CV Accuracy Std': std_accuracy,\n",
    "            'CV Precision Mean': mean_precision,\n",
    "            'CV Recall Mean': mean_recall,\n",
    "            'CV F1 Mean': mean_f1,\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame for easy visualization\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('Results/model_cv_results.csv', index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Training\n",
    "Here we will experiment on three different models: Logistic Regression, Decision Tree, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets once again based on results from RFE algorithm\n",
    "X_train = X_train[HSIC_feats]\n",
    "X_test = X_test[HSIC_feats]\n",
    "\n",
    "if not os.path.exists('Results/model_cv_results.csv'):\n",
    "    results_df = test_models_with_cv_metrics(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "original_results_df = pd.read_csv('Results/model_cv_results.csv')\n",
    "\n",
    "def plot_model_performance(results_df):\n",
    "    # Set the style of seaborn\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Melt the DataFrame to make it suitable for sns.barplot 2\n",
    "    results_melted = pd.melt(results_df, id_vars=[\"Model\"], var_name=\"Metric\", value_name=\"Score\",\n",
    "                             value_vars=[\"CV Accuracy Mean\", \"CV Precision Mean\", \"CV Recall Mean\", \"CV F1 Mean\"])\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.barplot(x=\"Score\", y=\"Model\", hue=\"Metric\", data=results_melted)\n",
    "    \n",
    "    plt.title('Model Performance Across Different Metrics')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Model')\n",
    "    plt.xlim(0.89, 1)  # Assuming the scores are normalized between 0 and 1\n",
    "    plt.legend(loc=\"upper\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Increase the font size\n",
    "    plt.xticks(fontsize=11)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_model_performance(original_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train):\n",
    "    # Define the models and hyperparameters to tune\n",
    "    models_params = {\n",
    "        'RandomForestClassifier': {\n",
    "            'model': RandomForestClassifier(),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "                'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "                'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "                'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],  # The number of features to consider when looking for the best split\n",
    "                'criterion': ['gini', 'entropy'],  # The function to measure the quality of a split\n",
    "                'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "            }\n",
    "        },\n",
    "        'XGBClassifier': {\n",
    "            'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [6, 10],\n",
    "                'learning_rate': [0.1, 0.01]\n",
    "            }\n",
    "        },\n",
    "        'CatBoostClassifier': {\n",
    "            'model': CatBoostClassifier(verbose=0),\n",
    "            'params': {\n",
    "                'iterations': [100, 200],\n",
    "                'depth': [6, 10],\n",
    "                'learning_rate': [0.1, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    tuning_results = []\n",
    "    \n",
    "    # Wrapping the loop with tqdm for progress visualization\n",
    "    for model_name, mp in tqdm(models_params.items(), desc=\"Tuning Models\"):\n",
    "        clf = GridSearchCV(mp['model'], mp['params'], cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        tuning_results.append({\n",
    "            'Model': model_name,\n",
    "            'Best Score': clf.best_score_,\n",
    "            'Best Params': clf.best_params_\n",
    "        })\n",
    "        \n",
    "    tuning_results = pd.DataFrame(tuning_results)\n",
    "    tuning_results.to_csv('Results/tuned_models.csv', index=False)\n",
    "    return tuning_results\n",
    "\n",
    "if not os.path.exists('Results/tuned_models.csv'):\n",
    "    tuning_results = tune_hyperparameters(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_comparison(original_results_df, tuned_results_df):\n",
    "    rename_mapping = {\n",
    "        'Random Forest': 'RandomForestClassifier'\n",
    "    }\n",
    "    original_results_df['Model'] = original_results_df['Model'].replace(rename_mapping)\n",
    "    tuned_results_df = tuned_results_df.rename(columns={'Best Score': 'CV Accuracy Mean (Tuned)'})\n",
    "    \n",
    "    # Check if there's any mismatch in model names\n",
    "    print(\"Original Models:\", original_results_df['Model'].unique())\n",
    "    print(\"Tuned Models:\", tuned_results_df['Model'].unique())\n",
    "    \n",
    "    # Attempt merging only if there are matching models\n",
    "    if not pd.merge(original_results_df, tuned_results_df, on='Model').empty:\n",
    "        # Merging the original and tuned scores for plotting\n",
    "        merged_df = pd.merge(original_results_df[['Model', 'CV Accuracy Mean']], \n",
    "                             tuned_results_df[['Model', 'CV Accuracy Mean (Tuned)']], \n",
    "                             on='Model', \n",
    "                             how='inner')\n",
    "        \n",
    "        if not merged_df.empty:\n",
    "            melted_df = pd.melt(merged_df, id_vars=[\"Model\"], var_name=\"Metric\", value_name=\"Score\")\n",
    "            \n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            sns.barplot(x='Model', y='Score', hue='Metric', data=melted_df)\n",
    "            plt.title('Model Performance Before and After Hyperparameter Tuning')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.legend(title='Metric')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Increase the font size\n",
    "            plt.xticks(fontsize=11)\n",
    "            \n",
    "            plt.ylim(0.95, 0.97)  # Assuming the scores are normalized between 0 and 1\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Merged DataFrame is empty. Check model names and column names.\")\n",
    "    else:\n",
    "        print(\"No matching models found between original and tuned results.\")\n",
    "\n",
    "tuned_models_df = pd.read_csv('Results/tuned_models.csv')\n",
    "plot_comparison(original_results_df, tuned_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class AveragingModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.scaler = StandardScaler()  # Initialize the scaler\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit each of the models with the scaled training data\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Get predictions from each model and average them\n",
    "        avg_proba = np.mean([model.predict_proba(X) for model in self.models], axis=0)\n",
    "        return avg_proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Convert averaged probabilities into final predictions\n",
    "        avg_proba = self.predict_proba(X)\n",
    "        final_predictions = np.argmax(avg_proba, axis=1)\n",
    "        return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the best performing hyperparameters\n",
    "import ast\n",
    "tuned_models_df = pd.read_csv('Results/tuned_models.csv')\n",
    "best_params = tuned_models_df.set_index('Model')['Best Params'].to_dict()\n",
    "\n",
    "RF_hyperparams = ast.literal_eval(best_params['RandomForestClassifier'])\n",
    "XGB_hyperparams = ast.literal_eval(best_params['XGBClassifier'])\n",
    "CAT_hyperparams = ast.literal_eval(best_params['CatBoostClassifier'])\n",
    "\n",
    "# Assign the best hyperparameters to the corresponding models\n",
    "RF = RandomForestClassifier(**RF_hyperparams)\n",
    "XGB = XGBClassifier(**XGB_hyperparams)\n",
    "CAT = CatBoostClassifier(**CAT_hyperparams, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the averaging wrapper model\n",
    "averaging_model = AveragingModel(models=[RF, XGB, CAT])\n",
    "\n",
    "# Convert X_train and y_train to pandas DataFrame\n",
    "X_train = pd.DataFrame(X_train, columns=HSIC_feats)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "# Scale the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the wrapper model with your training data\n",
    "averaging_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, averaging_model.predict(X_test_scaled))\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Get predictions on the test set\n",
    "predictions = averaging_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curves(X_test, y_test, models, model_names):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        # Get the probability scores of the positive class\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{name} (area = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Prepare models and names for plotting\n",
    "models = [RF, XGB, CAT, averaging_model]\n",
    "model_names = ['Random Forest', 'XGBoost', 'CatBoost', 'Averaging Model']\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curves(X_test_scaled, y_test, models, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "X = pd.DataFrame(X, columns=HSIC_feats)\n",
    "Y = pd.Series(Y)\n",
    "\n",
    "# Scale the X data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "if not os.path.exists('Models/averaging_model.joblib'):\n",
    "    averaging_model.fit(X, Y)\n",
    "    dump(averaging_model, 'Models/averaging_model.joblib')\n",
    "    \n",
    "# Export the scaler to a file\n",
    "if not os.path.exists('Models/scaler.joblib'):\n",
    "    dump(scaler, 'Models/scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing retrieval methods for params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def split_url(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    split_path = parsed_url.path.split('/')\n",
    "    \n",
    "    # Initialize components with -1 (indicating \"does not exist\")\n",
    "    components = {\n",
    "        \"domain\": -1,\n",
    "        \"directory\": -1,\n",
    "        \"file\": -1,\n",
    "        \"parameters\": -1\n",
    "    }\n",
    "    \n",
    "    # Extract domain\n",
    "    if parsed_url.netloc:\n",
    "        components[\"domain\"] = parsed_url.netloc\n",
    "    \n",
    "    # Extract file and directory\n",
    "    if split_path:\n",
    "        if len(split_path) > 1:\n",
    "            # Join all parts except the last one as directory\n",
    "            components[\"directory\"] = '/'.join(split_path[:-1])\n",
    "            if split_path[-1]:  # Check if the last part is not empty, indicating a file\n",
    "                components[\"file\"] = split_path[-1]\n",
    "        elif split_path[0]:  # Only one part, could be either directory or file\n",
    "            components[\"file\"] = split_path[0]\n",
    "    \n",
    "    # Extract parameters (query string)\n",
    "    if parsed_url.query:\n",
    "        # Parse query string into a dictionary\n",
    "        components[\"parameters\"] = parse_qs(parsed_url.query)\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def calculate_url_attributes(url):\n",
    "    components = split_url(url)  # Use your split_url function\n",
    "    domain = components['domain']\n",
    "    directory = components['directory']\n",
    "    file = components['file']\n",
    "    \n",
    "    # Initialize dictionary to store attributes\n",
    "    attributes = {\n",
    "        'qty_slash_url': url.count('/'),\n",
    "        'qty_dot_url': url.count('.'),\n",
    "        'length_url': len(url),\n",
    "        'file_length': len(file) if file != -1 else 0,\n",
    "        'qty_dot_directory': directory.count('.') if directory != -1 else 0,\n",
    "        'qty_dot_file': file.count('.') if file != -1 and '.' in file else 0,\n",
    "        'qty_slash_directory': directory.count('/') if directory != -1 else 0,\n",
    "        'qty_dot_domain': domain.count('.') if domain != -1 else 0,\n",
    "        'domain_length': len(domain) if domain else -1,\n",
    "        'qty_vowels_domain': sum(map(domain.lower().count, \"aeiou\")) if domain else -1\n",
    "    }\n",
    "\n",
    "    return attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_asn(ip_address, access_token='dbf71fc14e52e2'):\n",
    "    try:\n",
    "        response = requests.get(f'https://ipinfo.io/{ip_address}/json?token={access_token}')\n",
    "        data = response.json()\n",
    "        org_field = data.get('org', 'N/A')\n",
    "        asn = org_field.split(' ')[0] if org_field != 'N/A' else -1\n",
    "        \n",
    "        return asn[2:]\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving ASN for IP {ip_address}: {e}\")\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import dns.resolver\n",
    "import requests\n",
    "import socket\n",
    "import whois\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def perform_external_lookups(url):\n",
    "    features = {}\n",
    "    domain = split_url(url)['domain']\n",
    "    \n",
    "    # Initialize features to handle errors individually\n",
    "    features['time_domain_activation'] = -1\n",
    "    features['time_domain_expiration'] = -1\n",
    "    features['qty_mx_servers'] = -1\n",
    "    features['qty_nameservers'] = 0\n",
    "    features['ttl_hostname'] = -1\n",
    "    features['asn_ip'] = -1  # Placeholder for ASN lookup\n",
    "    features['time_response'] = -1\n",
    "    features['qty_redirects'] = -1\n",
    "    features['tls_ssl_certificate'] = -1\n",
    "    features['domain_spf'] = -1\n",
    "\n",
    "    try:\n",
    "        w = whois.whois(domain)\n",
    "        if w.creation_date:\n",
    "            if isinstance(w.creation_date, list):  # Handle multiple creation dates\n",
    "                w.creation_date = w.creation_date[0]\n",
    "            features['time_domain_activation'] = (datetime.now() - w.creation_date).days\n",
    "        if w.expiration_date:\n",
    "            if isinstance(w.expiration_date, list):  # Handle multiple expiration dates\n",
    "                w.expiration_date = w.expiration_date[0]\n",
    "            features['time_domain_expiration'] = (w.expiration_date - datetime.now()).days\n",
    "    except Exception as e:\n",
    "        print(f\"Error during WHOIS lookup: {e}\")\n",
    "    \n",
    "    try:\n",
    "        dns_resolver = dns.resolver.Resolver()\n",
    "        mx_records = dns_resolver.resolve(domain, 'MX')\n",
    "        features['qty_mx_servers'] = len(mx_records)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ns_records = dns_resolver.resolve(domain, 'NS')\n",
    "        features['qty_nameservers'] = len(ns_records)\n",
    "    except:\n",
    "        pass  # Feature remains -1 if error occurs\n",
    "\n",
    "    try:\n",
    "        features['ttl_hostname'] = dns_resolver.resolve(domain, 'A').rrset.ttl\n",
    "    except:\n",
    "        pass  # Feature remains -1 if error occurs\n",
    "\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        response = requests.get(url, timeout=100)\n",
    "        end_time = datetime.now()\n",
    "        features['time_response'] = (end_time - start_time).total_seconds()\n",
    "        features['qty_redirects'] = len(response.history)\n",
    "        features['tls_ssl_certificate'] = 1 if response.url.startswith('https') else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error making HTTP request: {e}\")\n",
    "\n",
    "    try:\n",
    "        txt_records = dns_resolver.resolve(domain, 'TXT')\n",
    "        spf_record = any(record.to_text().startswith('\"v=spf1') for record in txt_records)\n",
    "        features['domain_spf'] = 1 if spf_record else 0\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Resolve the domain to an IP address for the ASN lookup\n",
    "        ip_address = socket.gethostbyname(domain)\n",
    "        features['asn_ip'] = get_asn(ip_address)\n",
    "    except Exception as e:\n",
    "        print(f\"Error resolving domain to IP for ASN lookup: {e}\")\n",
    "        features['asn_ip'] = -1  # Set to -1 if there's an error\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveData(URL: str) -> pd.DataFrame:\n",
    "    url_attributes = calculate_url_attributes(URL)\n",
    "    \n",
    "    # Get the external lookups using the domain\n",
    "    external_lookups = perform_external_lookups(URL)\n",
    "    data = {**url_attributes, **external_lookups}\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = retrieveData(\"https://apple.com\")\n",
    "\n",
    "display(test)\n",
    "\n",
    "# reorder the columns based on HSIC_feats\n",
    "test = test[HSIC_feats]\n",
    "\n",
    "# Scale the test data\n",
    "scaler = load('Models/scaler.joblib')\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "loaded_model = load('Models/averaging_model.joblib')\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Or get probability predictions\n",
    "probability_predictions = loaded_model.predict_proba(test_scaled)\n",
    "\n",
    "print(predictions, probability_predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4251f1f3f61588c16b90f5696d157f5798b8bb60c31fefc70356d5820835e374"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
